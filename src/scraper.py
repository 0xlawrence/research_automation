# src/scraper.py

import requests
from bs4 import BeautifulSoup

def fetch_article_content(url: str) -> str:
    """
    æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰è¨˜äº‹æœ¬æ–‡ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦å–å¾—ã€‚
    - è¨˜äº‹ã®æœ¬æ–‡ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™ã€‚
    
    Args:
        url (str): ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®URLã€‚
    
    Returns:
        str: è¨˜äº‹æœ¬æ–‡ã®ãƒ†ã‚­ã‚¹ãƒˆã€‚
    """
    try:
        # HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
        response = requests.get(url, timeout=10)
        response.raise_for_status()  # HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ãŒã‚¨ãƒ©ãƒ¼ã®å ´åˆã€ä¾‹å¤–ã‚’ã‚¹ãƒ­ãƒ¼

        # BeautifulSoupã§HTMLã‚’è§£æ
        soup = BeautifulSoup(response.content, "html.parser")

        # è¨˜äº‹æœ¬æ–‡ã‚’æŠ½å‡ºã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆä¾‹: <div class="article-body">ï¼‰
        # å„ã‚µã‚¤ãƒˆã®HTMLæ§‹é€ ã«åˆã‚ã›ã¦èª¿æ•´ãŒå¿…è¦
        article_body = soup.find("div", class_="article-body")  # ä¾‹: `<div class="article-body">`
        if article_body:
            return article_body.get_text(strip=True)

        # ä»–ã®å¯èƒ½æ€§ï¼ˆä¾‹: <p> ã‚¿ã‚°å†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã™ã¹ã¦çµåˆï¼‰
        paragraphs = soup.find_all("p")
        if paragraphs:
            return "\n".join([p.get_text(strip=True) for p in paragraphs])

        # æœ¬æ–‡ãŒå–å¾—ã§ããªã‹ã£ãŸå ´åˆ
        return "è¨˜äº‹æœ¬æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"

    except requests.exceptions.RequestException as e:
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é–¢é€£ã®ã‚¨ãƒ©ãƒ¼å‡¦ç†
        print(f"HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return "HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"

    except Exception as e:
        print(f"âŒ Content Scraping Failed [URL: {url[:30]}...]")
        print(f"ğŸ› ï¸ Error Type: {type(e).__name__}")
        print(f"ğŸ’¡ Troubleshooting: Check site structure or robots.txt")
        return "Content unavailable: scraping failed"

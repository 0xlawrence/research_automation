import requests
import json
from datetime import datetime
import re
from typing import List, Dict, Any, Optional
from urllib.parse import urlparse

def fetch_hackernews_top_stories(limit: int = 30) -> List[int]:
    """
    HackerNews APIã‹ã‚‰æœ€æ–°ã®ãƒˆãƒƒãƒ—ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’å–å¾—ã—ã¾ã™ã€‚
    
    Args:
        limit: å–å¾—ã™ã‚‹è¨˜äº‹æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ30ï¼‰
    
    Returns:
        List[int]: ã‚¹ãƒˆãƒ¼ãƒªãƒ¼IDã®ãƒªã‚¹ãƒˆ
    """
    try:
        url = "https://hacker-news.firebaseio.com/v0/topstories.json"
        response = requests.get(url)
        response.raise_for_status()
        
        # æœ€å¤§500ä»¶ã®IDãƒªã‚¹ãƒˆãŒè¿”ã£ã¦ãã‚‹ã®ã§ã€æŒ‡å®šã•ã‚ŒãŸä»¶æ•°ã«åˆ¶é™
        story_ids = response.json()
        return story_ids[:limit]
    except Exception as e:
        print(f"âŒ Error fetching HackerNews top stories: {str(e)}")
        return []

def extract_domain(url: str) -> str:
    """
    URLã‹ã‚‰ãƒ‰ãƒ¡ã‚¤ãƒ³åã‚’æŠ½å‡ºã—ã¾ã™ã€‚
    
    Args:
        url: è¨˜äº‹ã®URL
    
    Returns:
        str: ãƒ‰ãƒ¡ã‚¤ãƒ³åï¼ˆä¾‹: 'example.com'ï¼‰
    """
    try:
        parsed_url = urlparse(url)
        domain = parsed_url.netloc
        # www.ã‚’å‰Šé™¤
        if domain.startswith('www.'):
            domain = domain[4:]
        return domain
    except:
        return "Unknown"

def fetch_story_details(story_id: int) -> Optional[Dict[str, Any]]:
    """
    HackerNews APIã‹ã‚‰ç‰¹å®šã®ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã®è©³ç´°ã‚’å–å¾—ã—ã¾ã™ã€‚
    
    Args:
        story_id: ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ID
    
    Returns:
        Dict or None: ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æƒ…å ±ã®è¾žæ›¸ã¾ãŸã¯Noneï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ï¼‰
    """
    try:
        url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        response = requests.get(url)
        response.raise_for_status()
        
        story = response.json()
        
        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒã‚§ãƒƒã‚¯
        if not story or not story.get('title') or not story.get('url', story.get('text')):
            print(f"âš ï¸ Missing required fields for story ID {story_id}")
            return None
            
        # 'Ask HN'ã‚„'Show HN'ã®ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã¯'text'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ã—ã¦æŒã¤ã“ã¨ãŒã‚ã‚‹
        content = story.get('text', '')
        if not content and 'url' in story:
            content = f"Original URL: {story['url']}"
        
        # æ—¥ä»˜ã®å¤‰æ›ï¼ˆUnixã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‹ã‚‰Datetimeï¼‰
        publish_date = None
        if 'time' in story:
            try:
                publish_date = datetime.fromtimestamp(story['time'])
            except (TypeError, ValueError) as e:
                print(f"âš ï¸ Error parsing date for story ID {story_id}: {e}")
        
        # URLã‹ã‚‰ãƒ‰ãƒ¡ã‚¤ãƒ³åã‚’æŠ½å‡º
        url = story.get('url', f"https://news.ycombinator.com/item?id={story_id}")
        source = extract_domain(url)
        
        # RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®å½¢å¼ã«åˆã‚ã›ã¦è¾žæ›¸ã‚’ä½œæˆ
        return {
            'title': story.get('title', ''),
            'link': url,
            'url': url,
            'summary': content,
            'published': publish_date,
            'source': source,  # ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’è¨˜äº‹ã®ã‚½ãƒ¼ã‚¹ã¨ã—ã¦ä½¿ç”¨
            'score': story.get('score', 0),
            'comments': story.get('descendants', 0)
        }
    except Exception as e:
        print(f"âŒ Error fetching story details for ID {story_id}: {str(e)}")
        return None

def fetch_top_hackernews_stories(max_items: int = 5) -> List[Dict[str, Any]]:
    """
    HackerNews APIã‹ã‚‰ãƒˆãƒƒãƒ—ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã¨ãã®è©³ç´°ã‚’å–å¾—ã—ã¾ã™ã€‚
    
    Args:
        max_items: å–å¾—ã™ã‚‹æœ€å¤§è¨˜äº‹æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ5ä»¶ï¼‰
    
    Returns:
        List[Dict]: ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æƒ…å ±ã®ãƒªã‚¹ãƒˆ
    """
    print(f"\nðŸ” Checking HackerNews top stories")
    
    # ãƒˆãƒƒãƒ—ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã®IDã‚’å–å¾—ï¼ˆå¤šã‚ã«å–å¾—ã—ã¦ã€è©³ç´°ãƒ•ã‚§ãƒƒãƒã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã«ååˆ†ãªæ•°ã‚’ç¢ºä¿ï¼‰
    story_ids = fetch_hackernews_top_stories(limit=max_items * 3)
    
    if not story_ids:
        print("âŒ Failed to fetch HackerNews story IDs")
        return []
        
    stories = []
    fetched_count = 0
    
    for story_id in story_ids:
        # æŒ‡å®šã•ã‚ŒãŸæ•°ã®æœ‰åŠ¹ãªã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’å–å¾—ã§ããŸã‚‰çµ‚äº†
        if fetched_count >= max_items:
            break
            
        story = fetch_story_details(story_id)
        if story:
            stories.append(story)
            fetched_count += 1
    
    print(f"ðŸ“„ Found {len(stories)} valid stories from HackerNews")
    return stories 
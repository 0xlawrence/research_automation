#!/usr/bin/env python3
from datetime import datetime
import logging
from typing import List, Dict
import time
import os
from dotenv import load_dotenv
import sys  # sys ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤‰æ›´
from .config import RSS_FEEDS, USE_DEEPSEEK, OPENAI_API_KEY  

# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from .rss_fetch import fetch_all_rss_items
from .hackernews_fetch import fetch_top_hackernews_stories
from .ai_client import ai_client  # çµ±ä¸€ã•ã‚ŒãŸAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

# OpenAIãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‹ã‚‰é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from .openai_utils import (
    summarize_text,
    categorize_article_with_ai,
    generate_detailed_summary,
    generate_insights_and_questions,
    process_article_content,
    transform_title
)

# APIã®ä½¿ç”¨çŠ¶æ³ã‚’è¡¨ç¤º
print(f"Using {'DeepSeek' if USE_DEEPSEEK else 'OpenAI'} API for content processing")
if not USE_DEEPSEEK:
    print(f"OpenAI API Key configured: {bool(OPENAI_API_KEY)}")

from .scraper import fetch_article_content
from .notion_utils import (
    create_notion_page, 
    append_page_content, 
    update_notion_status, 
    get_pages_by_status
)
from .cache_utils import load_processed_urls, save_processed_url

def register_new_articles():
    """
    æ–°ã—ã„è¨˜äº‹ã‚’RSSãƒ•ã‚£ãƒ¼ãƒ‰ã¨HackerNewsã‹ã‚‰å–å¾—ã—ã€Notionã«ç™»éŒ²ã€‚
    1. å„RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰æœ€æ–°5ä»¶ã‚’ç¢ºèª
    2. HackerNewsã‹ã‚‰æœ€æ–°5ä»¶ã‚’ç¢ºèª
    3. æœªå‡¦ç†ã®URLã®ã¿ã‚’ç™»éŒ²
    4. AIå‡¦ç†ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æŒ‡å®š
    """
    print("\nğŸ“š Starting to fetch articles from RSS feeds...")
    processed_urls = load_processed_urls()
    
    total_checked = 0
    total_new = 0
    
    # RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ã®è¨˜äº‹å–å¾—
    for item in fetch_all_rss_items(RSS_FEEDS):
        total_checked += 1
        article_url = item.get("url") or item.get("link")
        if not article_url:
            print(f"âŒ No URL found in item: {item}")
            continue
            
        if article_url.strip() in processed_urls:
            print(f"â­ï¸ Already processed: {item['title']}")
            continue
            
        try:
            # è¨˜äº‹ã®è¦ç´„ã‚’ç”Ÿæˆ
            content = item.get("summary", "")
            if not content:
                print(f"âš ï¸ No content found for: {article_url}")
                continue
                
            summary = summarize_text(content)
            if not summary:
                continue
                
            # å…ƒè¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å¤‰æ›
            raw_title = item["title"]
            refined_title = transform_title(raw_title)
            
            # Notionãƒšãƒ¼ã‚¸ã‚’ä½œæˆ
            result = create_notion_page(
                title=refined_title,
                url=article_url,
                summary=summary,
                published_date=item.get("published"),
                source=item.get("source", "Unknown"),
                category=categorize_article_with_ai(raw_title, summary)
            )
            
            if result:  # Notionãƒšãƒ¼ã‚¸ã®ä½œæˆãŒæˆåŠŸã—ãŸå ´åˆã®ã¿URLã‚’ä¿å­˜
                save_processed_url(article_url.strip())
                total_new += 1
                print(f"âœ… Registered: {item['title']}")
            else:
                print(f"âš ï¸ Failed to create Notion page for: {item['title']}")
            
        except Exception as e:
            print(f"âŒ Error processing: {article_url} - {str(e)}")
    
    # HackerNewsã‹ã‚‰ã®è¨˜äº‹å–å¾—
    print("\nğŸ“š Fetching articles from HackerNews...")
    for item in fetch_top_hackernews_stories(max_items=10):  # ä¸€æ™‚çš„ã«10ä»¶ã«å¢—ã‚„ã™
        total_checked += 1
        article_url = item.get("url") or item.get("link")
        if not article_url:
            print(f"âŒ No URL found in HackerNews item: {item}")
            continue
            
        if article_url.strip() in processed_urls:
            print(f"â­ï¸ Already processed: {item['title']}")
            continue
            
        try:
            # è¨˜äº‹ã®è¦ç´„ã‚’ç”Ÿæˆ
            content = item.get("summary", "")
            if not content:
                # HackerNewsã®å ´åˆã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒç©ºã®å ´åˆãŒã‚ã‚‹ãŸã‚ã€æœ€ä½é™ã®æƒ…å ±ã‚’è¿½åŠ 
                content = f"HackerNews Score: {item.get('score', 0)}, Comments: {item.get('comments', 0)}"
                
            summary = summarize_text(content)
            if not summary:
                # è¦ç´„ã«å¤±æ•—ã—ãŸå ´åˆã§ã‚‚ã€æœ€ä½é™ã®æƒ…å ±ã§è¦ç´„ã‚’ä½œæˆ
                summary = f"HackerNews discussion with {item.get('comments', 0)} comments and {item.get('score', 0)} points."
                
            # å…ƒè¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å¤‰æ›
            raw_title = item["title"]
            refined_title = transform_title(raw_title)
            
            # Notionãƒšãƒ¼ã‚¸ã‚’ä½œæˆ
            result = create_notion_page(
                title=refined_title,
                url=article_url,
                summary=summary,
                published_date=item.get("published"),
                source=item.get("source", "Unknown"),  # ãƒ‰ãƒ¡ã‚¤ãƒ³åã‚’ã‚½ãƒ¼ã‚¹ã¨ã—ã¦ä½¿ç”¨
                category=categorize_article_with_ai(raw_title, summary)
            )
            
            if result:  # Notionãƒšãƒ¼ã‚¸ã®ä½œæˆãŒæˆåŠŸã—ãŸå ´åˆã®ã¿URLã‚’ä¿å­˜
                save_processed_url(article_url.strip())
                total_new += 1
                print(f"âœ… Registered from HackerNews: {item['title']}")
                
                # AIå‡¦ç†ã‚’è‡ªå‹•çš„ã«é–‹å§‹ã™ã‚‹å ´åˆã¯ã“ã“ã§ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
                update_notion_status(result["id"], "Processing")
            else:
                print(f"âš ï¸ Failed to create Notion page for HackerNews item: {item['title']}")
            
        except Exception as e:
            print(f"âŒ Error processing HackerNews item: {item.get('title', 'Unknown')} - {str(e)}")
    
    print(f"\nğŸ“Š Summary:\n- Checked articles: {total_checked}\n- New articles registered: {total_new}")

def process_pending_articles():
    """
    `Processing` ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®è¨˜äº‹ã‚’å‡¦ç†ã€‚
    HackerNewsã®è¨˜äº‹ã‚‚å«ã‚ã¦å‡¦ç†ã—ã¾ã™ã€‚
    """
    print("\nğŸ“ Fetching Notion pages with 'Processing' status...")
    pages = get_pages_by_status("Processing")
    
    if not pages:
        print("No articles with 'Processing' status found.")
        return
        
    print(f"Found {len(pages)} articles to process.")
    
    for page in pages:
        try:
            # Notionãƒšãƒ¼ã‚¸ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‹ã‚‰å¿…è¦ãªæƒ…å ±ã‚’å–å¾—
            title = page["properties"]["Name"]["title"][0]["text"]["content"]
            url = page["properties"]["URL"]["url"]
            
            print(f"\nğŸ” Processing: {title}")
            
            # å‡¦ç†ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
            update_notion_status(page["id"], "Processing")
            print("Successfully updated status to: Processing")
            
            # è¨˜äº‹æœ¬æ–‡ã‚’å–å¾—
            content = fetch_article_content(url)
            if not content:
                print(f"âš ï¸ No content fetched for: {title}")
                update_notion_status(page["id"], "Error")
                continue
                
            print(f"Fetched {len(content)} characters of content")
            
            # è¨˜äº‹ãŒé•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚ã‚‹
            if len(content) > 50000:
                print("Content too long, truncating to 50000 characters")
                content = content[:50000] + "...[truncated due to length]"
            
            # process_article_contentã‚’ä½¿ç”¨ã—ã¦å‡¦ç†
            print("Generating initial summary...")
            processing_result = process_article_content(page["id"], content)
            
            if processing_result:
                update_notion_status(page["id"], "Completed")
                print(f"âœ… Successfully processed: {title}")
            else:
                update_notion_status(page["id"], "Error")
                print(f"âš ï¸ Failed to process: {title}")
            
            # APIåˆ¶é™ã‚’è€ƒæ…®ã—ã¦å¾…æ©Ÿ
            time.sleep(3)
            
        except Exception as e:
            print(f"âŒ Error in process_pending_articles: {e}")
            if 'page' in locals():
                update_notion_status(page["id"], "Error")

def main():
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†
    1. æ–°ã—ã„è¨˜äº‹ã®ç™»éŒ²
    2. ä¿ç•™ä¸­ã®è¨˜äº‹ã®å‡¦ç†
    """
    # DryRunãƒ¢ãƒ¼ãƒ‰ã®åˆ¤å®š
    if "--dry-run" in sys.argv:
        print("âš ï¸ Running in DRY RUN mode - no actual API calls will be made")
        return
        
    register_new_articles()
    process_pending_articles()

if __name__ == "__main__":
    main()
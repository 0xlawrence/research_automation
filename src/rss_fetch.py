#src/rss_fetch.py

import feedparser
from datetime import datetime
from html.parser import HTMLParser
import requests  # è¿½åŠ 


class HTMLTagRemover(HTMLParser):
    """
    HTMLã‚¿ã‚°ã‚’é™¤å»ã™ã‚‹ã‚¯ãƒ©ã‚¹
    """
    def __init__(self):
        super().__init__()
        self.text = []

    def handle_data(self, data):
        self.text.append(data)

    def get_clean_text(self):
        return ''.join(self.text)


def remove_html_tags(html_content: str) -> str:
    """
    HTMLã‚¿ã‚°ã‚’é™¤å»ã™ã‚‹é–¢æ•°
    Args:
        html_content (str): HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    Returns:
        str: HTMLã‚¿ã‚°ãŒé™¤å»ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    """
    parser = HTMLTagRemover()
    parser.feed(html_content)
    return parser.get_clean_text()


def clean_domain(url: str) -> str:
    """
    URLã‹ã‚‰ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ‰ãƒ¡ã‚¤ãƒ³åã‚’æŠ½å‡ºã™ã‚‹ã€‚
    - www.ã‚’é™¤å»
    - substackã®å ´åˆã¯ã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’ä¿æŒ
    - ãã®ä»–ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯é€šå¸¸é€šã‚Šå‡¦ç†
    """
    domain = url.split('/')[2]
    
    # www.ã‚’é™¤å»
    if domain.startswith('www.'):
        domain = domain[4:]
    
    # ã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’ä¿æŒã™ã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒªã‚¹ãƒˆ
    keep_subdomain_domains = ['substack.com']
    
    # ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒkeep_subdomain_domainsã®ã„ãšã‚Œã‹ã§çµ‚ã‚ã‚‹å ´åˆã€ã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’ä¿æŒ
    for keep_domain in keep_subdomain_domains:
        if domain.endswith(keep_domain):
            return domain
    
    # ãã‚Œä»¥å¤–ã®å ´åˆã¯ã€www.ã®ã¿ã‚’é™¤å»ã—ã¦è¿”ã™
    return domain


def fetch_rss_items(feed_url: str, max_items: int = 5):
    """
    æŒ‡å®šã—ãŸRSSãƒ•ã‚£ãƒ¼ãƒ‰URLã‹ã‚‰æœ€æ–°ã®è¨˜äº‹ã‚’å–å¾—ã—ãƒªã‚¹ãƒˆã¨ã—ã¦è¿”ã™ã€‚
    Args:
        feed_url: RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®URL
        max_items: å–å¾—ã™ã‚‹æœ€å¤§è¨˜äº‹æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ5ä»¶ï¼‰
    """
    try:
        print(f"\nğŸ” Checking feed: {feed_url}")
        feed = feedparser.parse(feed_url)
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰ã®å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆã®ãƒã‚§ãƒƒã‚¯
        if hasattr(feed, 'status') and feed.status >= 400:
            print(f"âŒ Error fetching feed: HTTP status {feed.status}")
            return []
            
        # ãƒ•ã‚£ãƒ¼ãƒ‰ãŒç©ºã¾ãŸã¯ç„¡åŠ¹ãªå ´åˆã®ãƒã‚§ãƒƒã‚¯
        if not hasattr(feed, 'entries') or not feed.entries:
            print(f"âš ï¸ No entries found in feed")
            return []
            
        items = []
        source = clean_domain(feed_url)
        
        print(f"ğŸ“„ Found {len(feed.entries[:max_items])} recent entries")
        
        for entry in feed.entries[:max_items]:
            try:
                # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å­˜åœ¨ç¢ºèª
                if not entry.get('title') or not entry.get('link'):
                    print(f"âš ï¸ Missing required fields in entry")
                    continue
                
                # æ—¥ä»˜ã®å‡¦ç†
                published = entry.get('published', '')
                published_dt = None
                if published and hasattr(entry, 'published_parsed'):
                    try:
                        published_dt = datetime(*entry.published_parsed[:6])
                    except (TypeError, AttributeError):
                        print(f"âš ï¸ Error parsing date for: {entry.get('title')}")
                
                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å–å¾—
                content = (entry.get('description', '') or 
                         entry.get('summary', '') or 
                         entry.get('content', [{'value': ''}])[0]['value'])
                
                clean_content = remove_html_tags(content)
                
                items.append({
                    'title': entry.title,
                    'link': entry.link,
                    'summary': clean_content,
                    'published': published_dt,
                    'source': source
                })
                
            except Exception as entry_error:
                print(f"âŒ Error processing entry: {str(entry_error)}")
                continue
                
        return items
        
    except Exception as e:
        print(f"âŒ Error fetching RSS feed: {str(e)}")
        return []


def fetch_all_rss_items(feed_urls):
    """
    è¤‡æ•°ã®RSSãƒ•ã‚£ãƒ¼ãƒ‰URLã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ã—ã¦ã¾ã¨ã‚ã¦è¿”ã™ã€‚
    Args:
        feed_urls (list): RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®URLãƒªã‚¹ãƒˆ
    Returns:
        list: è¨˜äº‹æƒ…å ±ã‚’ã¾ã¨ã‚ãŸãƒªã‚¹ãƒˆ
    """
    all_items = []
    for url in feed_urls:
        if not isinstance(url, str):
            print(f"Invalid URL: {url} (type: {type(url)})")
            continue

        print(f"Fetching articles from: {url}")
        try:
            items = fetch_rss_items(url, max_items=5)
            all_items.extend(items)
        except Exception as e:
            print(f"Error fetching articles from {url}: {e}")
    return all_items
